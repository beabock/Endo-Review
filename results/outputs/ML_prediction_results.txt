Log file for applying models to the full dataset
Generated by: scripts/03_prediction/apply_models_to_full_dataset.R
Analysis run on: 2025-11-23 17:59:30 

Memory optimization utilities loaded successfully!
Available functions:
  ‚Ä¢ monitor_memory() - Check memory usage
  ‚Ä¢ aggressive_gc() - Free memory
  ‚Ä¢ create_sparse_dtm() - Memory-efficient text matrices
  ‚Ä¢ process_large_dataset() - Chunked processing
  ‚Ä¢ optimize_dataframe() - Compress data structures
=== ENDOPHYTE SYSTEMATIC REVIEW: FULL DATASET CLASSIFICATION ===
Pipeline: Relevance ‚Üí Presence/Absence Classification
Models: glmnet (relevance), weighted ensemble (P/A)
Memory optimization: Enabled

Column name harmonization mapping defined.
Step 1: Loading and preparing full dataset...
  Loaded 21891 abstracts from full dataset
Loaded in training data to avoid predicting on labeled abstracts
  Excluded 462 training abstracts
  Final dataset: 21429 abstracts for prediction
Data loading and preparation: 4.45 sec elapsed

Step 2: Relevance Classification...
  Creating document-term matrix...
Creating sparse DTM from 21429 documents
  Tokenizing text...
  Filtering terms (min frequency: 2 )...
  Limiting to top 15000 features
  Creating sparse matrix...
  DTM created: 21429 documents √ó 15000 terms
  Sparsity: 99.5 %
  DTM created: 21429 documents √ó 15000 terms
  Loading trained relevance model...
  Performing memory-efficient feature alignment...
  Feature alignment summary:
  - Common features: 6930 
  - Missing features: 2701 
  - Total training features: 9631 
  Adding 2701 missing features...
  Feature alignment complete: 9631 features
‚ö†Ô∏è High memory usage in Before relevance predictions : 4.78 GB
   Consider running gc() or processing in smaller chunks
‚ö†Ô∏è High memory usage. Using chunked prediction.
  Predicting relevance...
  Large dataset detected. Processing in chunks of 1000 documents
  Processing documents 1 to 1000 
  Processing documents 1001 to 2000 
  Processing documents 2001 to 3000 
  Processing documents 3001 to 4000 
  Processing documents 4001 to 5000 
  Processing documents 5001 to 6000 
  Processing documents 6001 to 7000 
  Processing documents 7001 to 8000 
  Processing documents 8001 to 9000 
  Processing documents 9001 to 10000 
  Processing documents 10001 to 11000 
  Processing documents 11001 to 12000 
  Processing documents 12001 to 13000 
  Processing documents 13001 to 14000 
  Processing documents 14001 to 15000 
  Processing documents 15001 to 16000 
  Processing documents 16001 to 17000 
  Processing documents 17001 to 18000 
  Processing documents 18001 to 19000 
  Processing documents 19001 to 20000 
  Processing documents 20001 to 21000 
  Processing documents 21001 to 21429 
‚ö†Ô∏è High memory usage in After relevance predictions : 4.78 GB
   Consider running gc() or processing in smaller chunks
  Relevance classification results:
# A tibble: 3 √ó 4
  threshold        Irrelevant Relevant Uncertain
  <chr>                 <int>    <int>     <int>
1 relevance_loose        1982    19447         0
2 relevance_medium       1603    18911       915
3 relevance_strict        963    16882      3584
  Selected 19447 relevant abstracts for P/A classification
Relevance classification: 351.16 sec elapsed

Step 3: Presence/Absence Classification...
‚ö†Ô∏è High memory usage in Before P/A classification : 4.8 GB
   Consider running gc() or processing in smaller chunks
‚ö†Ô∏è High memory usage detected. Consider reducing dataset size or using chunked processing.
  Creating memory-efficient P/A DTM with unigrams and bigrams...
  Creating sparse DTM for unigrams...
Creating sparse DTM from 19447 documents
  Tokenizing text...
  Filtering terms (min frequency: 2 )...
  Limiting to top 8000 features
  Creating sparse matrix...
  DTM created: 19447 documents √ó 8000 terms
  Sparsity: 99.1 %
  Creating bigrams...
  Adding 2 documents with no valid bigrams as zero rows
  Combining unigrams and bigrams...
  Combined DTM: 19447 documents √ó 120245 features
‚ö†Ô∏è High memory usage in After P/A DTM creation : 4.9 GB
   Consider running gc() or processing in smaller chunks
‚ö†Ô∏è High memory usage after DTM creation. Consider further reducing max_features.
  Memory-efficient P/A DTM created: 19447 documents √ó 120245 terms
  Sparsity: NA %
  Loading trained P/A models...
  Training vocabulary size: 7492 features
  Performing memory-efficient feature alignment...
  Feature alignment summary:
  - Common features: 6050 
  - Missing features: 1442 
  - Total training features: 7492 
  Adding 1442 missing features in batches...
  P/A feature alignment complete: 7492 features
  Memory usage after alignment:
‚ö†Ô∏è High memory usage in After feature alignment : 5.49 GB
   Consider running gc() or processing in smaller chunks
  Applying memory-efficient ensemble predictions...
‚ö†Ô∏è High memory usage in Before ensemble predictions : 5.49 GB
   Consider running gc() or processing in smaller chunks
‚ö†Ô∏è High memory usage. Consider processing in chunks.
  Large dataset detected. Processing in chunks of 1000 documents
  Processing documents 1 to 1000 
  Processing documents 1001 to 2000 
  Processing documents 2001 to 3000 
  Processing documents 3001 to 4000 
  Processing documents 4001 to 5000 
  Processing documents 5001 to 6000 
  Processing documents 6001 to 7000 
  Processing documents 7001 to 8000 
  Processing documents 8001 to 9000 
  Processing documents 9001 to 10000 
  Processing documents 10001 to 11000 
  Processing documents 11001 to 12000 
  Processing documents 12001 to 13000 
  Processing documents 13001 to 14000 
  Processing documents 14001 to 15000 
  Processing documents 15001 to 16000 
  Processing documents 16001 to 17000 
  Processing documents 17001 to 18000 
  Processing documents 18001 to 19000 
  Processing documents 19001 to 19447 
  Combining chunk results...
  Ensemble predictions complete
‚ö†Ô∏è High memory usage in After ensemble predictions : 5.5 GB
   Consider running gc() or processing in smaller chunks
  P/A classification results:
# A tibble: 6 √ó 4
  method             Absence Presence Uncertain
  <chr>                <int>    <int>     <int>
1 pa_loose               827    18620         0
2 pa_medium              539    18219       689
3 pa_strict              111    16788      2548
4 pa_super_strict         10    14994      4443
5 threshold_ensemble     331    19116         0
6 weighted_ensemble       89    19358         0
  Performing final memory cleanup...
‚ö†Ô∏è High memory usage in End of P/A classification : 5.5 GB
   Consider running gc() or processing in smaller chunks
Presence/Absence classification: 1329.97 sec elapsed
  P/A classification completed successfully!
  Memory optimization preserved system stability.

Step 4: Saving results...
‚ö†Ô∏è High memory usage in Before results processing : 5.5 GB
   Consider running gc() or processing in smaller chunks
‚ö†Ô∏è High memory usage before results processing. Performing aggressive cleanup...
  Saving main results...

=== GENERATING ADDITIONAL MANUSCRIPT OUTPUTS FOR PUBLICATION ===
‚úì Saved temporal trends analysis for manuscript
‚úì Saved model performance summary for manuscript
‚úì Saved confidence calibration analysis for manuscript
‚úì Saved threshold comparison analysis for manuscript
‚úì Saved top predictive features analysis for manuscript

=== GENERATING MANUSCRIPT FIGURES FOR PREDICTION ANALYSIS ===
‚úì Saved prediction distribution summary for manuscript
‚úì Saved confidence distribution plot for manuscript
‚úì Saved threshold analysis for manuscript Methods section
‚úì Saved classification examples for manuscript
‚úì Saved processing statistics for manuscript
  Saving filtered subsets...
  Creating summary statistics...
Saving results: 33.45 sec elapsed

=== CLASSIFICATION COMPLETE ===
Results saved to results/ directory:
- full_dataset_predictions.csv: Complete results with all classifications
- relevant_abstracts_with_pa_predictions.csv: Relevant abstracts with P/A predictions
- irrelevant_uncertain_abstracts.csv: Abstracts likely not relevant for review
- classification_summary.txt: Summary statistics and recommendations
- manuscript_*.csv/txt: 10+ manuscript-ready outputs for publication
- plots/manuscript_confidence_distribution.png: Confidence visualization

Key Statistics:
- Total abstracts processed: 21429 
- Relevant abstracts (loose): 19447 
- Presence classifications: 19358 
- Absence classifications: 89 
- Uncertain classifications: 0 

Recommended workflow:
1. Use weighted ensemble results (final_classification) as primary screening
2. Manually review all 'Absence' classifications to avoid missing relevant studies
3. Spot-check 'Presence' classifications for quality assurance
4. Time permitting, review 'Uncertain' classifications

Pipeline complete! üéâ
